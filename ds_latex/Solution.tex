

\chapter{Methodology}
This chapter presents our methodogolical approach and describes its elements in detail.
Section 4.1 gives an overview of the methodological approach used to evaluate and compare the models. 
Section 4.2 gives an introduction to supervised learning and presents the different models considered in the comparison in detail.
Section 4.3 focusses on feature selection techniques applied in our computational study.

\section{Algorithms}

This section shortly introduces the statistical framework of supervised learning and then proceeds with the detailed examination of how each approach and algorithm included in our comparison, namely linear models, tree-based ensembles and neural networks, function. 

\subsection{Framework}
As inputs, supervised learning algorithms receive \textbf{training data} in form of a finite set $ S = \{({x}_{2}, y_2), ({x}_{2}, y_2), \dots, ({x}_{n}, y_n)\}$ of N pairs from $ \mathcal{X} \times \mathcal{Y} $ where $ x_i $ is a \textbf{sample} described by $ p $ features, and is associated to it's corresponding \textbf{label} $ y_i $.
They return a \textbf{hypothesis} $ h: \mathcal{X} \to \mathcal{Y} $ that aims to predict the corresponding label $ y \in Y $ for any $ x \in X $, but especially for unseen samples or i.e. \textbf{test data} $ x \notin S $.
We further assume a joint probability distribution $ P $ over $ \mathcal{X} $ and $ \mathcal{Y} $ each pair is identically and independently distributed according to. 
Thus, $ h(x) $ can be treated as a random variable being conditionally distributed with $ P(y | x) $ for a given $ x $, and not as a deterministic function of $ x $. 
The prediction accuracy of $ h $ is measured with a  \textbf{loss function} $ L : Y \times Y \to \mathbb{R}^{\geq 0}$.
For a given pair $ ({x}_i, y_i) $, the loss function $ L(y_i, \hat{y}_i) $ represents how far a prediction $ \hat{y_i} $ for the i-th sample $ x_i $ is away from the actual corresponding label $ y_i $. 
The average loss across all $ (x,y) \in S $ is called \textbf{empirical risk}.
Thus, the goal of a supervised learning algorithm is to find the optimal hypothesis $ h^* $ for which the overall empirical risk is minimal: 
\begin{equation}
	h^* = \argmin_{h \in H} \dfrac{1}{n} \sum_{i=1}^{n} L(h(x_i), y_i).
\end{equation}

Predictions can generally be made for either \textbf{regression} or \textbf{classification} problems which differ in the nature of their target variable. 
While regression is used to predict continuous targets $ Y = \mathbb{R} $, classification predicts discrete $ Y $. The arrival time estimation problem is a regression task since we are estimating (continuous) arrival times.
Although various supervised learning models use different mathematical procedures to predict targets, all of them follow the principle of induction, meaning that general rules are inductively inferred from training data. This is also called \textbf{generalization}.  
In order for a model to generalize well, two phenomena have to be avoided: \textbf{Underfitting} and \textbf{Overfitting}. 
Poor predictions on both the training and test data indicate that a model is underfitted. Very good prediction accuracy for training samples $ x \in S $, but poor accuracy on test samples $ x \notin S $ indicate that a model is overfitted.
In supervised learning, this dilemma is known as the \textbf{bias-variance tradeoff}. 
An overfitted model is characterized by rather being low in bias and high in variance, whereas the same goes vice versa for underfitted models. Models high in bias and low in variance are rather simple, but cannot capture complex mappings due to high bias, e.g. linear models. On the other hand, models low in bias and high in variance are able to capture complex patterns in the data which is the reason why they fit given training data $ S $ very well. The problem with models high in variance arises when predictions are performed on independent samples $ x \notin S $, because the better the model is fitted to $ S $, the higher becomes the variability of prediction performance on independent samples. 
The key challenge therefore lies in finding a model balancing this tradeoff well and thus having good and consistent prediction quality especially on test data.

For that reason, we consider diverse models covering the wide spectrum of model complexities to discover the suitable options for the EAT. 
 
\subsection{Linear Models}

Linear regression models assume that every $ y $ can be predicted with a linear combination of inputs, i.e. the training data $ S $. In the following, we will concisely explain how linear regression works. For further explanations, the reader is refered to \cite{friedman2001elements}.

Let $ \textbf{X}^{T} = (X_1, X_2, \dots, X_p) $ denote the input matrix containing $ p $ column vectors with N components each, and $ \hat{Y} = (y_1, y_2, \dots, y_n)$ the column vector with the corresponding real-valued outputs. A simple linear regression model is a linear combination of coefficients $ \mathbf{w} = (w_0, \dots, w_p) $ and column vectors $ X $:
\begin{equation}\label{eq:linearmodel}
h(X) = w_0 + \sum_{j=1}^{p} X_jw_j
\end{equation}
with coefficient $ w_0 $ representing the bias term. The feature vectors $ X \in \textbf{X}^{T} $ can be of any form and be arbitrarily transformed since linear models only assume linearity in their coefficients, and not in their inputs.
A common method of approximating the hypothesis in linear regression is \textit{least squares}, which seeks to find parameters that minimize the squared differences across all N pairs the training set $ S $:
\begin{equation}\label{eq:argminlinear}
\argmin_{w} \sum_{i=1}^{N}(y_i - w_0 - \sum_{j=1}^{p} x_{ij}w_j)^2. 
\end{equation}
The coefficients resulting from the least squares minimization in \ref{eq:argminlinear} are then employed in \ref{eq:linearmodel}, which can be used to predict labels for given samples. The larger a coefficient, the more does its corresponding feature impact the model outcome. 
Linear models are the better choice when it is known that the ground truth mapping between the independent and dependent variables of our model is linear since the model assumes linearity, is easy to implement and fast to train. The flipside of the coin is that more complex patterns can not be fully captured by linear models since it is always assumed that the underlying mapping of the given problem is linear which might not be the case for more complex scenarios. Nevertheless, we consider the presented linear regression model in our comparison for the former reasons and rather use it as a control and benchmark instance.
We will use the linear regression algorithm implemented in \textit{Scikit-Learn}, a library providing numerous methods from the field of data science and machine learning written in Python and proposed by \cite{scikit-learn}.

\subsection{Tree-based Ensembles}

In the following, we will present the tree-based ensembles we consider in our comparison. We again refer to the explanations done in \cite{friedman2001elements}.
As shown in the literature review, tree-based ensembles like GBDTs and Random Forests are popular in the field of arrival time estimation. Ensemble learning is motivated by the idea that \textit{base learners} (synonym to \textit{model} or \textit{hypothesis}) produce rather inaccurate results on their own, but form an accurate prediction model when they are combined. For tree-based ensembles, decision trees are used as base learners. 
Generally speaking, decision trees partition the feature space into distinct regions and then, in the case of regression, fit a real-valued prediction to each region. 
A single decision tree is given by
\begin{equation}
h(x) = \sum_{m=1}^{M} c_m I(x \in R_m)
\end{equation}
where $ c_m $ is the constant representing the prediction for all samples belonging to the m-th terminal region $ R_m $. Adopting the mean squared error as the loss metric, the predicted constant $ \hat{c}_m $ for each terminal region $ R_m $ is the average of all $ y $ for each $ x \in R_m $:
\begin{equation}
	\hat{c}_m =  \dfrac{1}{N_{R_m}}\sum_{x_i \in R_m}^{} y_i
\end{equation}
with $ N_{Rm} $ being the number of samples in terminal region $ R_m $. Given this, the decision tree algorithm needs to find optimal binary splits. Solving this analytically is computationally infeasible. Therefore, decision trees are constructed in a greedy fashion.
Let $ j $ be the split variable, representing the feature based on which the split of the feature space is done on, and $ s $ its associated split threshold.
The initial partition creates two disjoint regions
\begin{equation}
	R_1 (j,s) = \{X | X_j < s \}, R_{2} = \{X | X_j > s\}.
\end{equation}
The algorithm seeks optimal $ j $ and $ s $ by finding constants that minimize the overall loss, which in our case is measured with the mean squared error, for both terminal regions $ R_1 $ and $ R_2 $:
\begin{equation}\label{minjs}
	\min_{j,s} \bigg [\min_{\hat{c}_1} \sum_{x_i \in R_1(j,s)} \dfrac{1}{2}(y_i - \hat{c}_{2})^2 + \min_{\hat{c}_2} \sum_{x_i \in R_{2}(j,s)} \dfrac{1}{2}(y_i - \hat{c}_{2})^2\bigg]
\end{equation}
Having found the optimal split variable and split threshold, this procedure - also known as the classification and regression tree (CART) algorithm - is repeated for each terminal region until a termination condition holds, e.g. a fixed maximal depth for the tree or a lower bound wrt. number of samples in each terminal region. When trees are build according to the procedure we just explained, decision trees grow level-wise. Note that other ways to build decision trees such as the \textit{best-first} approach presented in \cite{BestFirst} where trees grow leaf wise do exist. 

However, decision trees have several shortcomings. First of all, decision trees tend to do a good job of predicting labels for samples used as training data. When it comes to independent samples, i.e. test data, they are performing poorly, which indicates high variance and overfitting. We thus turn to  decision tree ensembles, which basically combine several regression trees and aim to reduce the variance of the decision trees in their respective own ways. 
The two ensemble learning approaches relevant for our work are \textbf{bagging}, shorthand for \textit{bootstrap aggregating}, and \textbf{boosting}. 
Algorithms belonging to bagging first create bootstrap data sets $ S^{*} $ by randomly drawing samples with replacement from training data S, fit each $ S^{*b} $ a hypothesis $ h^{*b}(x) $ and take the average across all hypotheses as the bagging estimate: 
\begin{equation}\label{average}
	h_{bag}(x) = \dfrac{1}{B} \sum_{b=1}^{B} h^{*b}(x).
\end{equation}
Since decision trees are low in bias and high in variance due to their ability to capture complex mappings, bagging is motivated by the idea that averaging across all base learners benefits from the strong law of large numbers and therefore leads to a reduction in variance. 
Random Forests belong to the class of bagging algorithms and work as follows:
For an arbitrarily chosen and fixed amount of estimators $ B \in \mathbb{N}$, we create bootstrap data sets $ S^{*b} $ with $ b = 1, \dots, B $ of size $ N $ from the training data $ S $. The b-th regression tree $ T_b $ is grown to $ S^{*b} $ by first selecting a subset of $ m $ features with $ m \leq p $, and then growing a regression tree according to CART as described above.
Having grown each bootstrap data set $ S^{*b} $ its own regression tree $ T_b $, random forest for regression averages across all hypotheses of each regression tree by applying equation \ref{average} and returns
\begin{equation}
h^{B}_{rf}(x) = \dfrac{1}{B} \sum_{b=1}^{B} T_b(x)
\end{equation}
as the bagging estimate.
\newline
\newline
TODO: Random Forest Wahl motivieren, Boosting einleiten
\newline
\newline
The first step in GBDT is to initialize a model with a constant $ \hat{c} $ that minimizes the overall loss across all training samples.
\begin{equation}
h_{0}(x) =  \argmin_{\hat{c}} \sum_{i=1}^n L(y_i,\hat{c}) \label{gbdt_step1}
\end{equation}
Using mean squared error as the loss metric, the predictor minimizing the overall loss is simply the average of all $ y $.
Given the initial model $ h_0(x) $, M regression trees can now be build sequentially as shown in the following. 
The first step computes pseudo-residuals $ r_{im} $ between the prediction value of the former learner for the i-th sample $ h_{m-1}(x) $ and the actual i-th target for the m-th tree $ y_i $ by deriving the loss function w.r.t to $ h_{m-1}(x) $: 
\begin{equation}
	r_{im}\ = - \bigg[\dfrac{\delta L(y_i, h(x_i))}{\delta h(x_i)}\bigg]_{h(x) = h_{m-1}(x)}
\end{equation} 
for every i-th example.
In the second step, the algorithm fits a regression tree to every $ r_{im} $ and creates disjoint regions $ R_{jm} $ for $j = 1, ..., J_m$. 
In the third step, we compute: 
\begin{equation}
	\hat{c}_{jm} = \argmin_{\hat{c}} \sum_{x_i \in R_{ij}} L(y_i, h_{m-1}(x_i) + \hat{c})
\end{equation}
for every terminal region j in the m-th tree. 
Given this, we now update the former weak learner with the new one:
\begin{equation}
	h_m(x) = h_{m-1}(x) + \alpha \sum_{j=1}^{J_m} \hat{c}_{m}I(x \in R_{jm})
\end{equation}
where $ \alpha $ is the learning rate, and $ I(\cdot) $ is the indicator function.
Hence, we use the GBDT implemetation of \textit{lightGBM}.


\section{Feature Selection}\label{sec:fs}

A significant role for the success of machine learning models in general plays the data we gather and how we make sense of it - i.e. how we turn raw data into informative features - since machine learning models can only learn from the data they receive for the learning process. Feature selection seeks two conflicting objectives: Minimize the amount of used features while maximizing the accuracy. Balancing this trade-off is crucial, since an increase in features leads to exponential increase in training time, but a decrease in features can mean that strong predictors are excluded although they would significantly contribute to the objective. Therefore, we intend to analyze how our models work on data sets with different features and especially of different dimensionality. We select our features with the two aforementioned conflicting objectives in mind.  
Since we operate on the same problem and the same raw data as \cite{Hildebrandt2020_EAT}, we employ the data model proposed in their paper in our work too. As we have seen in the literature review, their data model includes temporal, spatial, routing and processing features and is of low dimensionality with a feature space of 12 features. 

\section{Design of Experiments}

The primary goal of this study is to find the most suitable model for the given arrival time estimation problem. 
To attain the best possible solution for this problem, it is necessary to analyze the presented mdoels from different points of view since it is not always clear which model does the best job in predicting arrival times as our literature review shows. 
While \cite{Zhu2020_OFCTE_DL} use deep learning as a means of estimating arrival respectively delivery times for restaurant meal delivery, \cite{Hildebrandt2020_EAT} turn to GBDTs on a quite similar problem setting, whereas \cite{Liu2018_LM_PLM} conclude that linear models are the better choice overall. 
For that reason, we will conduct experiments that will allow us to analyze the behaviour of the considered models and their parameters. 
Further challenge also lies in the selection of a well suited data model for the given problem which is highly non-trivial. 
Almost every related work shown in the literature review crafts features manually with the help of human domain expertise, and rightfully so: Raw data usually characterized by high dimensionality and sparse information, we aim to find the right balance for the two conflicting objectives of minimizing the feature space and thus reducing the dimensionality on the hand, while maximizing the informative value each considered feature contributes to the objective on the other hand. 
To demonstrate how models perform on different datasets and to find out which data model delivers more promising results, we decide to conduct our experiments upon the two data models presented in section \ref{sec:fs}, with one data model being high in dimensionality and the other one having a significantly smaller feature space.
In real-world scenarios, data can be corrupted by external influences, either caused by human or machine error. 
To see how those corruptions affect the model performances, we analyze supervised learning models also wrt. their robustness. 

Our experimental pipeline is designed as follows:
\begin{enumerate}
	\item \textbf{Finding Sufficient Sample Sizes:} The more data is available for training, the better the generalization will be. However, this conflicts with training times since there are more calculations to make when there is more data. Therefore, we seek a good balance for the tradeoff between sample size and accuracy. We do this by defining a number of training runs and assigning each run sample sizes from an interval with evenly spaced values. This experiment returns the sufficient sample size for each model, and additionally gives us information on how the models perform on solely manually set hyperparameters.  
	\item \textbf{Hyperparameter Optimization (HPO):} 
\end{enumerate}

