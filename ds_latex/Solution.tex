

\chapter{Solution Approach}
This chapter presents the solution approach.
Section 4.1 motivates our problem. 
Section 4.2 discusses the selected features.
Section 4.3 explains the algorithms considered in the comparison in detail.
Section 4.4 defines the process by which we evaluate the algorithms in order to assess the quality of each algorithm.

\section{Motivation}

We motivate this paper by answering the following questions:
Why is machine learning useful and widely-common for esitmation tasks? 
Which technical problems are encountered when using other approaches? 
Which machine learning algorithms are appropriate for our problem? Why bother comparing them? 

\section{Algorithms}
This section gives a short introduction to the conceptual framework of supervised learning and then examines the algorithms included in our comparison. For further research, the reader is refered to \cite{Bishop} and \cite{SLFoundations}.
\newline
\newline
Supervised learning models receive a finite sequence 
$S = \{(x, y) | x \in X, y \in Y\}$ of length $N \in \mathbb{N}$ as inputs where $x_{i \in N}\in X$ is an \textbf{observation} and $y_i \in Y$ its corresponding \textbf{target}. They aim for the approximation of an \textbf{hypothesis} $h: X \to Y$ that describes the underlying patterns of $X$ accurately w.r.t. a \textbf{loss function} $L: Y \times Y \to \mathbb{R}$ and predicts a target of a given observation. Loss functions represent the deviance of $h(x_i)$ for a given $x_i$ and its correspondent target $y_i$ by which we can measure how accurate the predictions of a model are. An optimal approximation function $h_{opt}$ is obtained by finding the minimal loss. $h$ is then used to predict any $y_i \in Y$ for a corresponding $x_i \in X$.  
\newline
Predictions can generally be made for either a \textbf{regression} or \textbf{classification} problem, which differ in the nature of their target variables. While regression is used to predict continuous targets $Y$, classification finds its use in the prediction of discrete $Y$. In machine learning terms, the arrival time estimation problems classifies as a regression task since we are estimating continuous time points. 
Although various supervised learning models use different mathematical procedures to predict targets, all of them follow the principle of induction, meaning that general rules are inductively inferred from the  observations. This is also called \textbf{generalization}.  
In order for a model to generalize the data properly, two things have to be avoided: \textbf{Underfitting} and \textbf{Overfitting}.


\subsection{GBDT}

\subsection{Random Forest}
- Example
- Formal Definition
\subsection{Neural Network}
- Example
- Formal Definition

\section{Feature Engineering}

- Raw Data 
- Manual feature selection
- Feature learning via deep autoencoder

\section{Evaluation} 
- Use MSE (and maybe more criteria) etc. to derive infos about accuracy
- Hyperparameter sensitivity analysis (Random Search) for robustness
- Variations in data set for robustness
--> Noise einfÃ¼hren 
--> Anzahl der Trainingsdaten
--> Andere Features
- Time for performance




















