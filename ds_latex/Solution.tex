

\chapter{Solution Approach}
This chapter presents the solution approach.
Section 4.1 motivates our problem. 
Section 4.2 discusses the selected features.
Section 4.3 explains the algorithms considered in the comparison in detail.
Section 4.4 defines the process by which we evaluate the algorithms in order to assess the quality of each algorithm.

\section{Motivation}



\section{Algorithms}
This section gives a short introduction to the conceptual framework of supervised learning and then examines the algorithms included in our comparison. For further research, the reader is referred to \cite{Bishop} and \cite{SLFoundations}.
\newline
\newline
Supervised learning models receive a finite sequence 
$S = \{(x_1, y) | x \in X, y \in Y\}$ of length $N \in \mathbb{N}$ as inputs where $x_i \in X$ with $i \in N$ is a \textbf{feature} (also called input or observation) and $y_i \in Y$ its corresponding \textbf{label} (also called output or target). 

They aim to fit a function $f: X \to Y$ to the underlying patterns of $X$ in an iterative fashion. 
The accuracy of a function is measured with \textbf{loss function} $L: Y \times Y \to \mathbb{R}$. $x_i$ that is passed to $h(x_i)$ from $y_i$ by which we can measure how accurate the predictions of a model are. $h_{opt}$ is then used to predict any target $h_{opt}(x_i)$ based on $x_i$.  
\newline
Predictions can generally be made for either a \textbf{regression} or \textbf{classification} problem, which differ in the nature of their target variables. While regression is used to predict continuous targets, classification finds its use in the prediction of discrete targets. The arrival time estimation problem is a regression task since we are estimating continuous values, which are the arrival times. 
Although various supervised learning models use different mathematical procedures to predict targets, all of them follow the principle of induction, meaning that general rules are inductively inferred from the observations. This is also called \textbf{generalization}.  
In order for a model to generalize the data properly, two things have to be avoided: \textbf{Underfitting} and \textbf{Overfitting}.



\subsection{Linear Models}

\subsection{Ensemble Learning}

Gradient Boosting
Random Forest
\subsection{Neural Networks}
- Example
- Formal Definition

\section{Feature Engineering}

- Raw Data 
- Manual feature selection
- 
- Feature learning via deep autoencoder

\section{Evaluation} 
- Use MSE, MAE, MAPE etc. to derive infos about accuracy
- Hyperparameter sensitivity analysis (Random Search) for robustness
- Variations in data set for robustness
--> Noise einfÃ¼hren 
--> Anzahl der Trainingsdaten
--> Andere Features
--> Runtime