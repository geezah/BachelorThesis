{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_IOWA_DATASET = True  # iowa dataset : true, simulation : false\n",
    "IOWA_PATH = '../../datasets/train_data_iowa.csv'\n",
    "SIMULATION_PATH = '../../datasets/datensatz_emre.csv'\n",
    "CSV_PATH = IOWA_PATH if IS_IOWA_DATASET else SIMULATION_PATH\n",
    "\n",
    "cols_to_skip = [\n",
    "    \"simulated_etd\",\n",
    "    \"restaurant_name\", \n",
    "    \"vehicle_name\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "meta = pd.read_csv(CSV_PATH, header=0, sep=';', usecols=lambda x : x not in cols_to_skip, nrows=1)\n",
    "\n",
    "s = meta.dtypes\n",
    "columns = []\n",
    "dtypes = {}\n",
    "\n",
    "#drop xy_pos AND every vehicle_i_*_j AND vehicle_total_route_* since they're not needed \n",
    "s.drop([k for k, _ in s.items() if (re.match(r\"[a-zA-Z_]+_[xy]_?\\d?\\d?\", k))], inplace=True)\n",
    "s.drop([k for k, _ in s.items() if (re.match(r\"vehicle_[0-9]+_[a-zA-Z_]+_[0-9]+\", k))], inplace=True)\n",
    "s.drop([k for k, _ in s.items() if (re.match(r\"vehicle_total_route[a-zA-Z0-9_]+\",k))], inplace = True)\n",
    "\n",
    "# These rows are nan rows\n",
    "s.drop([k for k, _ in s.items() if (re.match(r\"vehicle_route_to_customer_(pos|action|time_action)_23\",k))], inplace=True)\n",
    "\n",
    "for key in s.items():\n",
    "    if key[1] == \"int64\":\n",
    "        columns.append(key[0])\n",
    "        dtypes[key[0]] = \"int16\"\n",
    "        \n",
    "data = pd.read_csv(CSV_PATH, header=0, sep=\";\", usecols=columns, dtype=dtypes)\n",
    "data = data.apply(lambda x : pd.to_numeric(x, 'raise', 'signed'))\n",
    "\n",
    "pd.set_option(\"display.max_columns\", len(data.columns))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values?\n",
    "np.isnan(np.array(data)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables (\"insertion index\")\n",
    "#--> Later, when Florentins features are crafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection (by means of KDE probably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description\n",
    "def plot_histogram(x):\n",
    "    plt.hist(x, color='gray', alpha=0.5)\n",
    "    plt.title(f\"Histogram of {x.name}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features used in Hildebrandt et al. (2020):\n",
    "<ul>\n",
    "    <li>n_stops: sum(vehicle_route_to_customer_action_i = 1 or 2)</li>\n",
    "    <li>n_pickup_stops: sum(vehicle_route_to_customer_action_i = 1)</li>\n",
    "    <li>n_delivery_stops: sum(vehicle_route_to_customer_action_i = 2)</li>\n",
    "    <li>max_pre_shift: already given</li>    \n",
    "    <li>max_post_shift: already given</li>\n",
    "    <li>prep_time: sum(v_r_t_c_time_action_*) where v_r_t_c_action_i = 3 and v_r_t_c_pos_j == restaurant_location</li>\n",
    "    <li>order_time: already given</li>\n",
    "    <li>eta_pom: already given</li>\n",
    "    <li>customer_location: already given</li>\n",
    "    <li>restaurant_location: already given</li>\n",
    "</ul>\n",
    "Couple more? Brainstorming\n",
    "<ul>\n",
    "    <li>Split up eta_pom (i.e sum of estimated waiting times, delivery times (not anticipative)</li>\n",
    "    <li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define strings to identify needed columns for each feature we want to craft\n",
    "query_strings = {\n",
    "    \"n_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "        \n",
    "    \"n_pickup_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "    \n",
    "    \"n_delivery_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "    \n",
    "    \"prep_time\" : [\"vehicle_route_to_customer_time_action\", \n",
    "                   \"vehicle_route_to_customer_action\",\n",
    "                   \"order_time\", \"restaurant_location\", \"vehicle_route_to_customer_pos\"]\n",
    "}\n",
    "\n",
    "raw_feats = [\"location\", \"restaurant_location\", \"etd\", \"atd\", \"order_time\", \"max_pre_shift\", \"max_post_shift\", \"restaurant_queue\"]\n",
    "\n",
    "mask = pd.DataFrame()\n",
    "feats = pd.DataFrame()\n",
    "\n",
    "# First, add used raw features to feats\n",
    "for feat in raw_feats:\n",
    "    feats[feat] = data[feat]\n",
    "\n",
    "# Craft features and add to feats\n",
    "for key,value in query_strings.items():\n",
    "    \n",
    "    needed_columns = [col for col in data.columns if any(x in col for x in value)]\n",
    "    inp = data[needed_columns]\n",
    "    \n",
    "    if key == \"n_stops\":\n",
    "        for col in inp:\n",
    "            mask[col] = (inp[col] > 0) & (inp[col] < 3)\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_pickup_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 1\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_delivery_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 2\n",
    "            feats[key] = mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv(\"crafted_features.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted_features = pd.read_csv(\"crafted_features.csv\", sep=\";\", index_col=[0])\n",
    "crafted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models on both datasets (raw vs. crafted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'atd']\n",
    "y = data['atd'] - data['etd']\n",
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "X_crafted = crafted_features.loc[:, crafted_features.columns != 'atd']\n",
    "y_crafted = crafted_features['atd'] - crafted_features['etd']\n",
    "\n",
    "X_train_c, X_test_c, y_train_c , y_test_c = train_test_split(X_crafted,y_crafted, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"lgbm_rf\" : {\n",
    "        \"boosting_type\" : \"rf\",\n",
    "        \"objective\" : \"regression\",\n",
    "        \"learning_rate\" : 0.0005,\n",
    "        \"random_state\" : 42,\n",
    "        \"metrics\" : \"l2\",\n",
    "        \"bagging_freq\" : 10,\n",
    "        \"bagging_fraction\" : 0.8,\n",
    "    },\n",
    "    \n",
    "    \"lgbm_gbdt\" : {\n",
    "        \"boosting_type\" : \"gbdt\",\n",
    "        \"objective\" : \"regression\",\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"num_leaves\" : 20,\n",
    "        \"random_state\" : 42,\n",
    "        'metrics' : 'l2',    \n",
    "    },\n",
    "    \n",
    "    #\"lgbm_goss\" : {\n",
    "    #    \"boosting_type\" : \"goss\",\n",
    "    #    \"objective\" : \"regression\",\n",
    "    #     \"n_estimators\" : 500,\n",
    "    #    \"learning_rate\" : 0.05,\n",
    "    #    \"random_state\" : 42,\n",
    "    #    'metric' : 'l2'\n",
    "    #},\n",
    "    #\"lgbm_dart\" : {\n",
    "    #    \"boosting_type\" : \"dart\",\n",
    "    #    \"objective\" : \"regression\",\n",
    "    #     \"n_estimators\" : 500,\n",
    "    #    \"learning_rate\" : 0.05,\n",
    "    #    \"random_state\" : 42,\n",
    "    #    'metric' : 'l2'\n",
    "    #}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models on raw set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_set = lgb.Dataset(X,y)\n",
    "\n",
    "trained_models = []\n",
    "for model in params:\n",
    "    bst = lgb.cv(\n",
    "        params[model],\n",
    "        raw_set,\n",
    "        num_boost_round = 500,\n",
    "        early_stopping_rounds = 10,\n",
    "        verbose_eval = 5,\n",
    "        seed = 42,\n",
    "        return_cvbooster = True,\n",
    "        force_row_wise = True\n",
    "        stratified=False\n",
    "    )\n",
    "    trained_models.append(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                    SGDRegressor(\n",
    "                        max_iter=1000,\n",
    "                        validation_fraction=0.2,\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        verbose = 1)\n",
    "                   )\n",
    "reg.fit(X_crafted, y_crafted)\n",
    "mean_squared_error(y_test_c, reg.predict(X_test_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crafted_set = lgb.Dataset(X_crafted,y_crafted)\n",
    "\n",
    "trained_models = []\n",
    "for model in params:\n",
    "    bst = lgb.cv(\n",
    "        params[model],\n",
    "        train_set,\n",
    "        num_boost_round = 500,\n",
    "        early_stopping_rounds = 10,\n",
    "        verbose_eval = 5,\n",
    "        seed = 42,\n",
    "        return_cvbooster = True,\n",
    "        force_row_wise = True\n",
    "        stratified=False\n",
    "    )\n",
    "    trained_models.append(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK (Pytorch or Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Experiment with different architectures and techniques (i.e. MLP, Convolutional NNs (?) , ...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CancelOut(nn.Module):\n",
    "    '''\n",
    "    CancelOut Layer\n",
    "    \n",
    "    x - an input data (vector, matrix, tensor)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp, *kargs, **kwargs):\n",
    "        super(CancelOut, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.zeros(inp, requires_grad=True) + 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.weights.float())\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_encode):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.hidden_enc = nn.Linear(n_features, n_hidden)\n",
    "        self.encode = nn.Linear(n_hidden, n_encode)\n",
    "        self.hidden_dec = nn.Linear(n_encode, n_hidden)\n",
    "        self.decode = nn.Linear(n_hidden, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden_enc(x))\n",
    "        x = F.leaky_relu(self.encode(x))\n",
    "        x = F.leaky_relu(self.hidden_dec(x))\n",
    "        x = torch.sigmoid(self.decode(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        super(Model, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.hidden = nn.Linear(n_features, n_hidden)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.predict = nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.dropout(x))\n",
    "        x = F.leaky_relu(self.hidden(x))\n",
    "        x = F.leaky_relu(self.dropout(x))\n",
    "        x = self.predict(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from utils import *\n",
    "\n",
    "#Hyperparameter values DL\n",
    "LR = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "#reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Importing data.\")\n",
    "feature_list = [f for f in list(data.columns.values) if f != 'atd']\n",
    "\n",
    "etd_dataset = ETDData(data=data, feature_list=feature_list)\n",
    "split = DataSplit(etd_dataset, shuffle=True)\n",
    "trainloader, _, testloader = split.get_split(batch_size=50, num_workers=8)\n",
    "\n",
    "print(\"Data imported.\")\n",
    "print(\"Instanciating model.\")\n",
    "n_features = len(feature_list)\n",
    "n_hidden = math.ceil(n_features * (1 / 2))\n",
    "n_hidden_2 = math.ceil(n_hidden * (1 / 2))\n",
    "n_encode = math.ceil(n_hidden_2 * (1 / 2))\n",
    "\n",
    "ae = Autoencoder(\n",
    "        n_features=n_features,\n",
    "        n_hidden=n_hidden,\n",
    "        n_encode=n_encode,\n",
    "    )\n",
    "\n",
    "ae.to(device)\n",
    "criterion = nn.MSELoss()  # define your loss function and optimizer\n",
    "optimizer = optim.AdamW(ae.parameters(), lr=LR)\n",
    "\n",
    "print(\"Start training.\")\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE,\n",
    "                                verbose=True)  # TODO: Define your early stopping\n",
    "\n",
    "epochs = 100  # How many epochs do you want to train?\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = inputs.float().to(device)\n",
    "        #labels = labels.float().view(-1, 1).to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = ae(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    test_loss = 0\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            #labels = labels.float().view(-1, 1).to(device)\n",
    "            logps = ae.forward(inputs)\n",
    "            batch_loss = criterion(logps, inputs)\n",
    "            test_loss += batch_loss.item()\n",
    "    train_losses.append(running_loss / len(trainloader))\n",
    "    test_losses.append(test_loss / len(testloader))\n",
    "    print(f\"Epoch {epoch}/{epochs}.. \"\n",
    "            f\"Train loss: {running_loss / len(trainloader):.3f}.. \"\n",
    "            f\"Test loss: {test_loss / len(testloader):.3f}.. \")\n",
    "    early_stopping(test_loss / len(testloader), ae)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    ae.train()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.load_state_dict(torch.load('checkpoint.pt'))\n",
    "torch.save(ae, 'perceptron.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bachelor",
   "language": "python",
   "name": "bachelor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
