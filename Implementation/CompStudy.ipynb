{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from downcast import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "from model import Autoencoder, Regressor\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE FEATURE SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOWA_PATH = '../../datasets/train_data_iowa.csv'\n",
    "CF_PATH = \"../../datasets/crafted_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"location\",\n",
    "    \"location_x\", \"location_y\",\n",
    "    \"restaurant_location_x\", \"restaurant_location_y\", \n",
    "    \"order_time\",\n",
    "    \"etd\",\n",
    "    \"restaurant_queue\",\n",
    "    \"max_pre_shift\",\n",
    "    \"max_post_shift\",\n",
    "    \"restaurants_before_customer\",\n",
    "    \"customers_before_customer\",\n",
    "    \"len_vehicle_route_to_customer\",\n",
    "]\n",
    "\n",
    "for i in range(24):\n",
    "    features.append(f\"vehicle_route_to_customer_pos_x_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_pos_y_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_action_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_time_action_{i}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "start_time = time()\n",
    "\n",
    "usecols=[*features, \"atd\"]\n",
    "\n",
    "meta = pd.read_csv(IOWA_PATH, header=0, sep=\";\", usecols = usecols, nrows=2)\n",
    "meta = reduce(meta)\n",
    "dtypes = dict(meta.dtypes)\n",
    "\n",
    "raw = pd.read_csv(IOWA_PATH, header=0, sep=\";\", usecols = usecols, dtype=dtypes) \n",
    "\n",
    "print(f\"Elapsed time: {time() - start_time} seconds\")\n",
    "print(raw.info(verbose=False, memory_usage=\"deep\"))\n",
    "X_raw = raw.loc[:, raw.columns != 'atd']\n",
    "y_raw = raw['atd'] - raw['etd']\n",
    "\n",
    "pd.set_option(\"display.max_columns\", len(meta.columns))\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features used in Hildebrandt et al. (2020):\n",
    "<ul>\n",
    "    <li>n_stops: sum(vehicle_route_to_customer_action_i = 1 or 2)</li>\n",
    "    <li>n_pickup_stops: sum(vehicle_route_to_customer_action_i = 1)</li>\n",
    "    <li>n_delivery_stops: sum(vehicle_route_to_customer_action_i = 2)</li>\n",
    "    <li>max_pre_shift: already given</li>    \n",
    "    <li>max_post_shift: already given</li>\n",
    "    <li>prep_time: already given ( == restaurant_queue)</li>\n",
    "    <li>order_time: already given</li>\n",
    "    <li>eta_pom: already given</li>\n",
    "    <li>customer_location: already given</li>\n",
    "    <li>restaurant_location: already given</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define strings to identify needed columns for each feature we want to craft\n",
    "query_strings = {\n",
    "    \"n_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "        \n",
    "    \"n_pickup_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "    \n",
    "    \"n_delivery_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "}\n",
    "\n",
    "raw_feats = [\n",
    "    \"location_x\", \"location_y\",\n",
    "    \"restaurant_location_x\", \"restaurant_location_y\",\n",
    "    \"etd\", \n",
    "    \"atd\", \n",
    "    \"order_time\", \n",
    "    \"max_pre_shift\", \n",
    "    \"max_post_shift\", \n",
    "    \"restaurant_queue\",\n",
    "    \"restaurants_before_customer\", \n",
    "    \"customers_before_customer\",\n",
    "    \"len_vehicle_route_to_customer\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "mask = pd.DataFrame()\n",
    "feats = pd.DataFrame()\n",
    "\n",
    "# First, add used raw features to feats\n",
    "for feat in raw_feats:\n",
    "    feats[feat] = raw[feat]\n",
    "\n",
    "# Craft features and add to feats\n",
    "for key,value in query_strings.items():\n",
    "    \n",
    "    needed_columns = [col for col in raw.columns if any(x in col for x in value)]\n",
    "    inp = raw[needed_columns]\n",
    "    \n",
    "    if key == \"n_stops\":\n",
    "        for col in inp:\n",
    "            mask[col] = (inp[col] > 0) & (inp[col] < 3)\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_pickup_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 1\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_delivery_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 2\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"dropoff_time\" :\n",
    "        inp_copy = inp.drop([\"restaurant_location_x\", \"restaurant_location_y\"], axis=1)\n",
    "        actions = inp_copy[[f for f in inp_copy if \"vehicle_route_to_customer_action\" in f]] \n",
    "        \n",
    "        dropoff_times = []\n",
    "        \n",
    "        for index, row in actions.head(50).iterrows():\n",
    "            dropoff_action = []\n",
    "            for i, v in row.items():\n",
    "                if v == 4:\n",
    "                    dropoff_action.append(i)\n",
    "            customer_dropoff = dropoff_action[-1]\n",
    "            print(index)\n",
    "            dropoff_times.append(\n",
    "                inp_copy.at[index, f\"vehicle_route_to_customer_time_action_{customer_dropoff[-1]}\"]\n",
    "            )\n",
    "        dropoff_np = np.asarray(dropoff_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv(CF_PATH, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted = pd.read_csv(CF_PATH, sep=\";\", index_col=[0])\n",
    "\n",
    "X_crafted = crafted.loc[:, crafted.columns != 'atd']\n",
    "y_crafted = crafted['atd'] - crafted['etd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displot(data, xlabel, ylabel, filepath=None, kind=\"kde\", bw_adjust=2):\n",
    "    ax = sns.displot(data, \n",
    "            kind=kind,\n",
    "            bw_adjust=bw_adjust,\n",
    "            height=4, aspect=6/4,\n",
    "            legend=True)\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "    if filepath != None:\n",
    "        ax.savefig(filepath)\n",
    "    plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displot(\n",
    "    data = crafted[\"order_time\"], \n",
    "    xlabel = \"Order Time (in min)\", \n",
    "    ylabel = \"Frequency (relative)\", \n",
    "    filepath = \"Plots/order_time_dist.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displot(\n",
    "    data = crafted[\"atd\"]-crafted[\"etd\"],\n",
    "    xlabel = \"Delivery delay (in min)\",\n",
    "    ylabel = \"Frequency (relative)\",\n",
    "    filepath = \"Plots/delivery_delay.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displot(\n",
    "    data = crafted[\"restaurant_queue\"],\n",
    "    xlabel = \"Preparation time (in min)\",\n",
    "    ylabel = \"Frequency (relative)\",\n",
    "    filepath = \"Plots/prep_time.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    crafted[\"restaurant_queue\"],\n",
    "    kind=\"kde\",\n",
    "    bw_adjust = 3,\n",
    "    height = 4, aspect=6/4\n",
    ").savefig(\"Plots/prep_time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Scatterplot x Heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_locations = np.asarray(list(set(zip(raw.location_x, raw.location_y))))\n",
    "customer_locations_x = [t[0] for t in customer_locations]\n",
    "customer_locations_y = [t[1] for t in customer_locations]\n",
    "\n",
    "restaurant_locations = list(set(zip(raw.restaurant_location_x, raw.restaurant_location_y)))\n",
    "restaurant_locations_x = [t[0] for t in restaurant_locations]\n",
    "restaurant_locations_y = [t[1] for t in restaurant_locations]\n",
    "\n",
    "print(customer_locations.shape)\n",
    "plt.scatter(customer_locations_x, customer_locations_y, s=0.1)\n",
    "plt.scatter(restaurant_locations_x, restaurant_locations_y, s=10, marker=\"h\")\n",
    "plt.savefig(\"Plots/spatial_dist.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train() for NN, temporary in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(sample_sizes, results, title):\n",
    "    plt.xlabel(\"Sample size\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.plot(sample_sizes, results)\n",
    "    plt.savefig(f\"Plots/{title}.png\")\n",
    "\n",
    "def best_iteration(evals_result):\n",
    "    best = evals_result[0]\n",
    "    for i in evals_result:\n",
    "        if best > i:\n",
    "            best = i\n",
    "    return best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_train(X_train, y_train, X_test, y_test, params):\n",
    "    \n",
    "    train_set = lgb.Dataset(X_train,y_train)\n",
    "    val_set = lgb.Dataset(X_test, y_test, reference=train_set)\n",
    "    \n",
    "    evals_result = {}\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        train_set=train_set,\n",
    "        valid_sets=[val_set, train_set],\n",
    "        verbose_eval=5,\n",
    "        evals_result = evals_result,\n",
    "    )\n",
    "    best_mse = best_iteration(evals_result[\"valid_0\"][\"l2\"])\n",
    "    print(best_mse)\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crafted,y_crafted, train_size=0.8, random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\" : \"gbdt\",\n",
    "    \"metrics\" : \"l2\",\n",
    "    \"learning_rate\" : 0.02, \n",
    "    \"num_threads\"  : 6,\n",
    "    \"random_state\" : 42,\n",
    "    \"force_row_wise\" : True,\n",
    "    \"n_estimators\" : 10,\n",
    "    \"early_stopping_rounds\" : 20,\n",
    "}\n",
    "\n",
    "bst = ensemble_train(X_train, y_train, X_test, y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(model, data, feature_list, params):\n",
    "    \n",
    "    # Set the seed for reproducability\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Importing data.\")\n",
    "\n",
    "    etd_dataset = ETDData(data=data, feature_list=feature_list, objective=model.name)\n",
    "    split = DataSplit(etd_dataset, shuffle=True)\n",
    "    trainloader, _, testloader = split.get_split(batch_size=params[\"batch_size\"], num_workers=8)\n",
    "    \n",
    "    print(\"Start training.\")\n",
    "    patience = params[\"patience\"]\n",
    "    criterion = params[\"criterion\"]  \n",
    "    optimizer = params[\"optimizer\"]\n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=params[\"patience\"], verbose=True) \n",
    "    epochs = params[\"epochs\"]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.float().view(-1, model.view).to(device) \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().view(-1, model.view).to(device) \n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                test_loss += batch_loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss / len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss / len(testloader):.3f}.. \")\n",
    "        early_stopping(test_loss / len(testloader), model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        model.train()\n",
    "        \n",
    "    print('Finished Training')\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    model.save(model, 'perceptron.pth')\n",
    "    return model, abs(early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Sample size testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first part, we seek to examine the convergence behavior of our models and answer following question: How many samples are enough to train the model without ? \n",
    "We determine the answer to that question graphically. For that, we construct plots where the x-axis represents the number of samples used in the corresponding training instance, and the y-axis represents the corresponding L<sub>2</sub>-loss measured with the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.1: Tree-based ensembles: GBDT and RF (LightGBM Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Convergence Test for LightGBM's GBDT ###\n",
    "def sample_size_ensembles(X, y, params, title, start=1000, stop=100000, step=1000):\n",
    "    \n",
    "    sample_sizes = np.arange(start=start, stop=stop, step=step)\n",
    "    results = []\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=42)\n",
    "    \n",
    "    for rows in sample_sizes:\n",
    "        \n",
    "        train_set = lgb.Dataset(X_train[:rows],y_train[:rows])\n",
    "        val_set = lgb.Dataset(X_test, y_test, reference=train_set)\n",
    "        evals_result = {}\n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            train_set=train_set,\n",
    "            valid_sets=[val_set, train_set],\n",
    "            valid_names=[\"Validation error\", \"Train error\"],\n",
    "            verbose_eval=5,\n",
    "            evals_result = evals_result,\n",
    "        )\n",
    "        best_mse = best_iteration(evals_result[\"Validation error\"][\"l2\"])\n",
    "        results.append(best_mse)\n",
    "    plot_convergence(sample_sizes, results, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gbdt = {\n",
    "    \"boosting_type\" : \"gbdt\",\n",
    "    \"metrics\" : \"l2\",\n",
    "    \"learning_rate\" : 0.02, \n",
    "    \"num_threads\"  : 6,\n",
    "    \"random_state\" : 42,\n",
    "    \"force_row_wise\" : True,\n",
    "    \"n_estimators\" : 1000,\n",
    "    \"early_stopping_rounds\" : 20,\n",
    "}\n",
    "\n",
    "params_rf = {\n",
    "    \"boosting_type\" : \"rf\",\n",
    "    \"learning_rate\" : 0.02,\n",
    "    \"metrics\" : \"l2\",\n",
    "    \"n_estimators\" : 1000,\n",
    "    \"bagging_fraction\" : 0.632,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"feature_fraction\" : 0.632,\n",
    "    \"num_threads\"  : 6,\n",
    "    \"random_state\" : 42,\n",
    "    \"force_row_wise\" : True,\n",
    "    \"early_stopping\" : 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_ensembles(X_raw, y_raw, params_rf, \"RF_SampleSizeTest_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_ensembles(X_raw, y_raw, params_gbdt, \"GBDT_SampleSizeTest_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_ensembles(X_crafted, y_crafted, params_gbdt, \"GBDT_SampleSizeTest_Crafted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_ensembles(X_crafted, y_crafted, params_rf, \"RF_SampleSizeTest_Crafted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convergence test for Scikit-Learn's Linear Regression ###\n",
    "def sample_size_lr(X, y, title, start=1000, stop=100000, step=1000):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=42)\n",
    "    sample_sizes = np.arange(start=start, stop=stop, step=step)\n",
    "    results = []\n",
    "    \n",
    "    for rows in sample_sizes:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train[:rows], y_train[:rows])\n",
    "        mse = mean_squared_error(y_test, lr.predict(X_test))\n",
    "        print(f\"Sample size - error : {rows} - {mse}\")\n",
    "        results.append(mse)\n",
    "    plot_convergence(sample_sizes, results, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_lr(X_raw, y_raw, \"LR_SampleSize_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_lr(X_crafted, y_crafted, \"LR_SampleSize_Crafted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.3: Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_sizes = [1000,10000,100000]\n",
    "results = []\n",
    "\n",
    "n_features = len(features)\n",
    "n_hidden = math.ceil(n_features * (1 / 2))\n",
    "n_code = math.ceil(n_hidden * (1 / 2))\n",
    "\n",
    "ae = Autoencoder(n_features=n_features, n_hidden=n_hidden, n_code=n_code)\n",
    "slp = Regressor(n_features = n_features, n_hidden = n_hidden, n_output = 1)\n",
    "\n",
    "params = {\n",
    "        \"patience\" : 10,\n",
    "        \"criterion\" : nn.MSELoss(),\n",
    "        \"optimizer\" : optim.Adam(slp.parameters(), lr=0.0001),\n",
    "        \"epochs\" : 100,\n",
    "        \"batch_size\" : 50,\n",
    "}\n",
    "\n",
    "for rows in sample_sizes:\n",
    "    print(f\"Sample size {rows}\")\n",
    "    model, mse = train(slp, raw, features, params)\n",
    "    results.append(mse)\n",
    "    print(f\"Mean squared error: {mse} for sample size: {rows}\")\n",
    "plot_convergence(sample_sizes, results, \"nn_sample_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hyperparameter optimization (HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(study):\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"Value: {}\".format(trial.value))\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"{}: {}\".format(key, value))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO Objective functions for Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_trees(trial, X, y, mode):\n",
    "    \n",
    "    sample_size=100000\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "    train_set = lgb.Dataset(X_train[:sample_size],y_train[:sample_size])\n",
    "    valid_set = lgb.Dataset(X_test, y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"gbdt\" : {\n",
    "            \"boosting_type\" : \"gbdt\",\n",
    "            \"metric\" : \"l2\",\n",
    "            \"objective\" : \"regression\",\n",
    "            \"learning_rate\" : trial.suggest_uniform(\"learning_rate\", 0.01, 0.05),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 20, 80),\n",
    "            \"feature_fraction\" : trial.suggest_uniform(\"feature_fraction\", 0.1, 1.0),\n",
    "            \"num_leaves\" : trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "            \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\", 1, 40),\n",
    "            \"max_bin\" : 1000,\n",
    "            \"feature_pre_filter\" : False,\n",
    "            \"num_threads\"  : 6,\n",
    "            \"random_state\" : 42,\n",
    "            \"force_row_wise\" : True, \n",
    "            \"num_boost_round\": 1000,\n",
    "            \"early_stopping\" : 50,\n",
    "        },\n",
    "        \"rf\" : {\n",
    "            \"boosting_type\" : \"rf\",\n",
    "            \"metric\" : \"l2\", \n",
    "            \"objective\" : \"regression\",\n",
    "            \"n_estimators\" : trial.suggest_int(\"n_estimators\", 500, 1000),\n",
    "            \"learning_rate\" : trial.suggest_uniform(\"learning_rate\", 0.01, 0.05),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 20,60),\n",
    "            \"feature_fraction\" : trial.suggest_uniform(\"feature_fraction\", 0.50, 0.99),\n",
    "            \"bagging_fraction\" : trial.suggest_uniform(\"bagging_fraction\", 0.50, 0.99), \n",
    "            \"bagging_freq\" : trial.suggest_int(\"bagging_frequency\", 1, 20),\n",
    "            \"num_leaves\" : trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "            \"min_data_in_leaf\" : trial.suggest_int(\"min_child_samples\", 1, 40),\n",
    "            \"feature_pre_filter\" : False,\n",
    "            \"max_bin\" : 1000,\n",
    "            \"num_threads\"  : 6,\n",
    "            \"random_state\" : 42,\n",
    "            \"force_row_wise\" : True, \n",
    "            \"num_boost_round\": 1000,\n",
    "            \"early_stopping\" : 50,\n",
    "        }      \n",
    "    }\n",
    "    evals_result = {}\n",
    "    bst = lgb.train(\n",
    "        params[mode],\n",
    "        train_set=train_set,\n",
    "        valid_sets=[valid_set, train_set],\n",
    "        valid_names=[\"Validation error\", \"Train error\"],\n",
    "        verbose_eval=0,\n",
    "        evals_result = evals_result\n",
    "    )\n",
    "    best_mse = best_iteration(evals_result[\"Validation error\"][\"l2\"])\n",
    "    print(f\"Best iteration: {best_mse}\")\n",
    "    return best_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", \n",
    "    sampler=optuna.samplers.CmaEsSampler(seed=42),\n",
    "    pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
    ")\n",
    "start_time = time()\n",
    "study.optimize(lambda trial: hpo_trees(trial, X_raw, y_raw, \"gbdt\"), n_trials=100)\n",
    "print(f\"Elapsed time: {time() - start_time} seconds\")\n",
    "\n",
    "joblib.dump(study, \"gbdt_raw.pkl\")\n",
    "\n",
    "print_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.CmaEsSampler(seed=42))\n",
    "study.optimize(lambda trial: hpo_trees(trial, X_raw, y_raw, \"rf\"), n_trials=500)\n",
    "\n",
    "joblib.dump(study, \"rf_raw.pkl\")\n",
    "\n",
    "print_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = joblib.load(\"gbdt_raw.pkl\")\n",
    "print_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Introducing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
