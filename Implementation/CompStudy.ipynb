{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "\n",
    "from model import Autoencoder, Regressor\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE FEATURE SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOWA_PATH = '../../datasets/train_data_iowa.csv'\n",
    "SIMULATION_PATH = '../../datasets/datensatz_emre.csv'\n",
    "\n",
    "IS_IOWA_DATASET = True  # iowa dataset : true, simulation : false\n",
    "CSV_PATH = IOWA_PATH if IS_IOWA_DATASET else SIMULATION_PATH\n",
    "\n",
    "CF_PATH = \"../../datasets/crafted_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"location\",\n",
    "    \"location_x\", \"location_y\",\n",
    "    \"restaurant_location_x\", \"restaurant_location_y\", \n",
    "    \"order_time\",\n",
    "    \"etd\",\n",
    "    \"restaurant_queue\",\n",
    "    \"max_pre_shift\",\n",
    "    \"max_post_shift\",\n",
    "    \"restaurants_before_customer\",\n",
    "    \"customers_before_customer\",\n",
    "    \"len_vehicle_route_to_customer\",\n",
    "]\n",
    "\n",
    "for i in range(23):\n",
    "    features.append(f\"vehicle_route_to_customer_pos_x_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_pos_y_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_action_{i}\")\n",
    "    features.append(f\"vehicle_route_to_customer_time_action_{i}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "start_time = time()\n",
    "\n",
    "raw = pd.read_csv(CSV_PATH, header=0, sep=\";\", usecols=[*features, \"atd\"])\n",
    "X = raw.loc[:, raw.columns != 'atd']\n",
    "y = raw['atd'] - raw['etd']\n",
    "\n",
    "print(f\"Elapsed time: {time() - start_time} seconds\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", len(raw.columns))\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norm_dummy = (dummy - dummy.mean(axis=0)) / dummy.std(axis=0)\n",
    "#norm_dummy.columns.get_loc(\"vehicle_route_to_customer_pos_x_21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features used in Hildebrandt et al. (2020):\n",
    "<ul>\n",
    "    <li>n_stops: sum(vehicle_route_to_customer_action_i = 1 or 2)</li>\n",
    "    <li>n_pickup_stops: sum(vehicle_route_to_customer_action_i = 1)</li>\n",
    "    <li>n_delivery_stops: sum(vehicle_route_to_customer_action_i = 2)</li>\n",
    "    <li>max_pre_shift: already given</li>    \n",
    "    <li>max_post_shift: already given</li>\n",
    "    <li>prep_time: already given ( == restaurant_queue)</li>\n",
    "    <li>order_time: already given</li>\n",
    "    <li>eta_pom: already given</li>\n",
    "    <li>customer_location: already given</li>\n",
    "    <li>restaurant_location: already given</li>\n",
    "</ul>\n",
    "\n",
    "Weitere:\n",
    "<ul>\n",
    "    <li>Restaurants before customer : already given</li>\n",
    "    <li>Customers before customer : already given</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define strings to identify needed columns for each feature we want to craft\n",
    "query_strings = {\n",
    "    \"n_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "        \n",
    "    \"n_pickup_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "    \n",
    "    \"n_delivery_stops\" : [\"vehicle_route_to_customer_action\"],\n",
    "    \n",
    "    \"prep_time\" : [\"vehicle_route_to_customer_time_action\", \n",
    "                   \"vehicle_route_to_customer_action\",\n",
    "                   \"order_time\", \"restaurant_location\", \"vehicle_route_to_customer_pos\"]\n",
    "}\n",
    "\n",
    "raw_feats = [\n",
    "    \"location_x\", \"location_y\",\n",
    "    \"restaurant_location_x\", \"restaurant_location_y\",\n",
    "    \"etd\", \n",
    "    \"atd\", \n",
    "    \"order_time\", \n",
    "    \"max_pre_shift\", \n",
    "    \"max_post_shift\", \n",
    "    \"restaurant_queue\",\n",
    "    \"restaurants_before_customer\", \"customers_before_customer\"\n",
    "]\n",
    "\n",
    "mask = pd.DataFrame()\n",
    "feats = pd.DataFrame()\n",
    "\n",
    "# First, add used raw features to feats\n",
    "for feat in raw_feats:\n",
    "    feats[feat] = raw[feat]\n",
    "\n",
    "# Craft features and add to feats\n",
    "for key,value in query_strings.items():\n",
    "    \n",
    "    needed_columns = [col for col in raw.columns if any(x in col for x in value)]\n",
    "    inp = raw[needed_columns]\n",
    "    \n",
    "    if key == \"n_stops\":\n",
    "        for col in inp:\n",
    "            mask[col] = (inp[col] > 0) & (inp[col] < 3)\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_pickup_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 1\n",
    "            feats[key] = mask.sum(axis=1)\n",
    "    \n",
    "    if key == \"n_delivery_stops\": \n",
    "        for col in inp:\n",
    "            mask[col] = inp[col] == 2\n",
    "            feats[key] = mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv(CF_PATH, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted = pd.read_csv(CF_PATH, sep=\";\", index_col=[0])\n",
    "crafted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(crafted[\"order_time\"], \n",
    "            kind=\"kde\",\n",
    "            bw_adjust=1,\n",
    "            height=4, aspect=6/4,\n",
    "            legend=True).savefig(\"Plots/order_time_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(crafted[\"atd\"]-crafted[\"etd\"], \n",
    "            kind=\"kde\", \n",
    "            bw_adjust=2,\n",
    "            height=4, aspect=6/4).savefig(\"Plots/delivery_delay.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    crafted[\"restaurant_queue\"],\n",
    "    kind=\"kde\",\n",
    "    bw_adjust = 2,\n",
    "    height = 4, aspect=6/4\n",
    ").savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_locations = np.asarray(list(set(zip(raw.location_x, raw.location_y))))\n",
    "customer_locations_x = [t[0] for t in customer_locations]\n",
    "customer_locations_y = [t[1] for t in customer_locations]\n",
    "\n",
    "restaurant_locations = list(set(zip(raw.restaurant_location_x, raw.restaurant_location_y)))\n",
    "restaurant_locations_x = [t[0] for t in restaurant_locations]\n",
    "restaurant_locations_y = [t[1] for t in restaurant_locations]\n",
    "\n",
    "print(customer_locations.shape)\n",
    "plt.scatter(customer_locations_x, customer_locations_y, s=0.1)\n",
    "plt.scatter(restaurant_locations_x, restaurant_locations_y, s=10, marker=\"h\")\n",
    "plt.xlabel(\"latitude\")\n",
    "plt.ylabel(\"longitude\")\n",
    "plt.savefig(\"Plots/spatial_dist\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train() of NN, temporary in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, feature_list, params):\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Importing data.\")\n",
    "\n",
    "    etd_dataset = ETDData(data=data, feature_list=feature_list, objective=model.name)\n",
    "    split = DataSplit(etd_dataset, shuffle=True)\n",
    "    trainloader, _, testloader = split.get_split(batch_size=params[\"batch_size\"], num_workers=8)\n",
    "    \n",
    "    print(\"Start training.\")\n",
    "    patience = params[\"patience\"]\n",
    "    criterion = params[\"criterion\"]  # define your loss function and optimizer\n",
    "    optimizer = params[\"optimizer\"]\n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=params[\"patience\"], verbose=True) \n",
    "    epochs = params[\"epochs\"] # How many epochs do you want to train?\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.float().view(-1, model.view).to(device) \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().view(-1, model.view).to(device) \n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                test_loss += batch_loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss / len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss / len(testloader):.3f}.. \")\n",
    "        early_stopping(test_loss / len(testloader), model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        model.train()\n",
    "        \n",
    "    print('Finished Training')\n",
    "    #ae.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    #torch.save(ae, 'perceptron.pth')\n",
    "    return model, abs(early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Different sample sizes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first part, we seek to examine the convergence behavior of our models and answer following question: How many samples are enough to train the model without ? \n",
    "We determine the answer to that question graphically. For that, we construct plots where the x-axis represents the number of samples used in the corresponding training instance, and the y-axis represents the corresponding L<sub>2</sub>-loss measured with the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(sample_sizes, results, title):\n",
    "    plt.xlabel(\"Sample size\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.plot(sample_sizes, results)\n",
    "    plt.savefig(f\"Plots/{title}.png\")\n",
    "\n",
    "# Helper function for lightgbm\n",
    "def best_iteration(evals_result):\n",
    "    iterations = evals_result\n",
    "    small = iterations[0]\n",
    "    for i in iterations:\n",
    "        if small > i:\n",
    "            small = i\n",
    "     \n",
    "    return small    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.1: Tree-based ensembles: GBDT and RF (LightGBM Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Convergence Test for LightGBM's GBDT ###\n",
    "def gbdt_convergence(X, y, params, start=1000, stop=200000, step=1000, early_stopping_rounds=10):\n",
    "    \n",
    "    sample_sizes = np.arange(start=start, stop=stop, step=step)\n",
    "    results = []\n",
    "    \n",
    "    evals_result = {}\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "    \n",
    "    for rows in sample_sizes:\n",
    "        train_set = lgb.Dataset(X_train[:rows],y_train[:rows])\n",
    "        val_set = lgb.Dataset(X_test[:rows], y_test[:rows], reference=train_set)\n",
    "        \n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            train_set=train_set,\n",
    "            valid_sets=[val_set, train_set],\n",
    "            evals_result = evals_result,\n",
    "            early_stopping_rounds = early_stopping_rounds\n",
    "        )\n",
    "        best_iter = best_iteration(evals_result=dict(evals_result[\"valid_0\"])[\"l2\"])\n",
    "        print(f\"Best iteration: {best_iteration}\")\n",
    "        print(f\"Lower bound value: {bst.upper_bound()}\")\n",
    "        results.append(best_iter)\n",
    "    plot_convergence(sample_sizes, results, \"GBDT_Convergence\")\n",
    "\n",
    "\n",
    "params_gbdt = {\n",
    "    \"boosting_type\" : \"gbdt\",\n",
    "    \"metrics\" : \"l2\",\n",
    "    \"learning_rate\" : 0.02, \n",
    "    \"num_threads\"  : 6,\n",
    "    \"random_state\" : 42,\n",
    "    \"force_row_wise\" : True,\n",
    "    \"n_estimators\" : 1000,\n",
    "}\n",
    "\n",
    "params_rf = {\n",
    "    \"boosting_type\" : \"rf\",\n",
    "    \"metrics\" : \"l2\", \n",
    "    \"n_estimators\" : 1000,\n",
    "    \"bagging_fraction\" : 0.632,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"num_threads\"  : 6,\n",
    "    \"random_state\" : 42,\n",
    "    \"force_row_wise\" : True,\n",
    "}\n",
    "\n",
    "gbdt_convergence(X, y, params_rf, 10000, 11000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convergence test for Scikit-Learn's Linear Regression ###\n",
    "def lr_convergence(X, y, params=None, start=1000, stop=101000, step=1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "    sample_sizes = np.arange(start=start, stop=stop, step=step)\n",
    "    results = []\n",
    "    \n",
    "    for rows in sample_sizes:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train[:rows], y_train[:rows])\n",
    "        mse = mean_squared_error(y_test, lr.predict(X_test))\n",
    "        print(f\"Sample size - error: {rows} -> {mse}\")\n",
    "        results.append(mse)\n",
    "    plot_convergence(sample_sizes, results, \"LR_Convergence\")\n",
    "\n",
    "lr_convergence(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convergence test for Scikit-Learn's Random Forest Regressor ###\n",
    "def rf_convergence(X, y, params=None, start=1000, stop=101000, step=1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "    sample_sizes = np.arange(start=start, stop=stop, step=step)\n",
    "    results = []\n",
    "    \n",
    "    for rows in sample_sizes:\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train[:rows], y_train[:rows])\n",
    "        mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "        print(f\"Sample size - error: {rows} -> {mse}\")\n",
    "        results.append(mse)\n",
    "    plot_convergence(sample_sizes, results)\n",
    "    \n",
    "    \n",
    "params = {\n",
    "    \"n_estimators\" : 100,\n",
    "    \"n_jobs\" : 6,\n",
    "    \"verbose\" : 1,\n",
    "    \"random_state\" : 42,\n",
    "}\n",
    "\n",
    "rf_convergence(X, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.4: Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_sizes = np.arange(start=20000, stop=22000, step=2000)\n",
    "results = []\n",
    "\n",
    "n_features = len(features)\n",
    "n_hidden = math.ceil(n_features * (1 / 2))\n",
    "n_code = math.ceil(n_hidden * (1 / 2))\n",
    "\n",
    "ae = Autoencoder(n_features=n_features, n_hidden=n_hidden, n_code=n_code)\n",
    "slp = Regressor(n_features = n_features, n_hidden = n_hidden, n_output = 1)\n",
    "\n",
    "params = {\n",
    "        \"patience\" : 10,\n",
    "        \"criterion\" : nn.MSELoss(),\n",
    "        \"optimizer\" : optim.Adam(slp.parameters(), lr=0.0001),\n",
    "        \"epochs\" : 10,\n",
    "        \"batch_size\" : 50,\n",
    "}\n",
    "\n",
    "#for rows in sample_sizes:\n",
    "model, mse = train(slp, raw[:20000], features, params)\n",
    "#    results.append(mse)\n",
    "#plot_convergence(sample_sizes, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbdt_opt(trial, X, y, boosting_type):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "    train_set = lgb.Dataset(X_train,y_train)\n",
    "    valid_set = lgb.Dataset(X_test, y_test)\n",
    "    \n",
    "    params = {\n",
    "        \"gbdt\" : {\n",
    "            \"boosting_type\" : \"gbdt\",\n",
    "            \"metric\" : \"l2\",\n",
    "            \"objective\" : \"regression\",\n",
    "            \"learning_rate\" : trial.suggest_uniform(\"learning_rate\", 0.01, 0.05),\n",
    "            \"num_leaves\" : trial.suggest_int(\"num_leaves\", 30, 150),\n",
    "            \"min_child_samples\" : trial.suggest_int(\"min_child_samples\", 1, 20),\n",
    "            \"num_threads\"  : 6,\n",
    "            \"random_state\" : 42,\n",
    "            \"force_row_wise\" : True, \n",
    "            \"num_boost_round\": 1000,\n",
    "        },\n",
    "        \"rf\" : {\n",
    "            \n",
    "        }      \n",
    "    }\n",
    "    evals_result = {}\n",
    "    bst = lgb.train(\n",
    "        params[boosting_type],\n",
    "        train_set=train_set,\n",
    "        valid_sets=[valid_set, train_set],\n",
    "        valid_names=[\"Validation error\", \"Train error\"],\n",
    "        evals_result = evals_result,\n",
    "        verbose_eval = 5,\n",
    "    )\n",
    "    preds = bst.predict(X_test)\n",
    "    loss  = mean_squared_error(y_test, preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.CmaEsSampler(seed=42))\n",
    "study.optimize(lambda trial: gbdt_opt(trial, X, y, \"gbdt\"), n_trials=1)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_opt(trial, X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "    \n",
    "    lr = linear_model.LinearRegression(),\n",
    "    lr.fit(X_train, y_train)\n",
    "    return mean_squared_error(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Introducing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK (Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_code):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.name = \"ae\"\n",
    "        self.view = n_features\n",
    "        self.hidden_enc = nn.Linear(n_features, n_hidden)\n",
    "        self.encode = nn.Linear(n_hidden, n_code)\n",
    "        self.hidden_dec = nn.Linear(n_code, n_hidden)\n",
    "        self.decode = nn.Linear(n_hidden, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden_enc(x))\n",
    "        x = F.leaky_relu(self.encode(x))\n",
    "        x = F.leaky_relu(self.hidden_dec(x))\n",
    "        x = self.decode(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Regressor(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.name = \"regressor\"\n",
    "        self.view = n_output\n",
    "        self.hidden = nn.Linear(n_features, n_hidden)\n",
    "        self.predict = nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from utils import *\n",
    "    \n",
    "\n",
    "def train(model, data, feature_list, fit_params):\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Importing data.\")\n",
    "\n",
    "    etd_dataset = ETDData(data=data, feature_list=feature_list, objective=model.name)\n",
    "    split = DataSplit(etd_dataset, shuffle=True)\n",
    "    trainloader, _, testloader = split.get_split(batch_size=params[\"batch_size\"], num_workers=8)\n",
    "    \n",
    "    print(\"Start training.\")\n",
    "    patience = params[\"patience\"]\n",
    "    criterion = params[\"criterion\"]  # define your loss function and optimizer\n",
    "    optimizer = params[\"optimizer\"]\n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=params[\"patience\"], verbose=True) \n",
    "    epochs = params[\"epochs\"] # How many epochs do you want to train?\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.float().view(-1, model.view).to(device) \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().view(-1, model.view).to(device) \n",
    "                logps = model.forward(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                test_loss += batch_loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss / len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss / len(testloader):.3f}.. \")\n",
    "        early_stopping(test_loss / len(testloader), model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        model.train()\n",
    "        \n",
    "    print('Finished Training')\n",
    "    #ae.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    #torch.save(ae, 'perceptron.pth')\n",
    "    return model, abs(early_stopping.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
